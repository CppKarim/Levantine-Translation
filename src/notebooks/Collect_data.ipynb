{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download a file from a URL to the specified path.\"\"\"\n",
    "    try:\n",
    "        # Create parent directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "        \n",
    "        # Download the file\n",
    "        print(f\"Downloading {url} to {output_path}...\")\n",
    "        urllib.request.urlretrieve(url, output_path)\n",
    "        print(\"Download complete!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_filename_from_url(url):\n",
    "    \"\"\"Extract the filename from a URL.\"\"\"\n",
    "    path = urlparse(url).path\n",
    "    return os.path.basename(path) or \"download\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5033# to ../../Data...\n",
      "Error downloading file: [Errno 13] Permission denied: '../../Data'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5033#\"\n",
    "output = \"../../Data\"\n",
    "download_file(url,output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Load a text file and return a list of lines.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def load_ids_file(file_path):\n",
    "    \"\"\"\n",
    "    Load an .ids file and return a list of mappings.\n",
    "    \n",
    "    Each line in the file is expected to have the format:\n",
    "    source_file\\ttarget_file\\tsource_positions\\ttarget_positions\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with parsed mapping information\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 4:\n",
    "                mapping = {\n",
    "                    'source_file': parts[0],\n",
    "                    'target_file': parts[1],\n",
    "                    'source_positions': parts[2].split(),\n",
    "                    'target_positions': parts[3].split()\n",
    "                }\n",
    "                mappings.append(mapping)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "    return mappings\n",
    "\n",
    "def create_north_levantine_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Create a Hugging Face dataset from the North Levantine parallel corpus.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the dataset files\n",
    "    \n",
    "    Returns:\n",
    "        A DatasetDict containing the aligned parallel corpus\n",
    "    \"\"\"\n",
    "    # ISO codes for all languages in the dataset\n",
    "    languages = ['eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa']\n",
    "    \n",
    "    # Step 1: Discover and load all language files\n",
    "    text_data = {}\n",
    "    \n",
    "    # Find all language files in the directory\n",
    "    language_files = {}\n",
    "    for file in tqdm(os.listdir(data_dir), desc=\"Finding languages\"):\n",
    "        if file.endswith(tuple(languages)) and not file.endswith(\".ids\"):\n",
    "            lang_code = file.split(\".\")[-1]\n",
    "            language_files[lang_code] = os.path.join(data_dir, file)\n",
    "    \n",
    "    # Load discovered language files\n",
    "    for lang in tqdm(languages, desc=\"Loading language files\"):\n",
    "        if lang in language_files:\n",
    "            text_data[lang] = load_text_file(language_files[lang])\n",
    "            print(f\"Loaded {lang} file with {len(text_data[lang])} lines\")\n",
    "        else:\n",
    "            print(f\"Warning: File for language '{lang}' not found in {data_dir}\")\n",
    "    \n",
    "    # Step 2: Create individual datasets with index column\n",
    "    language_datasets = {}\n",
    "    for lang, lines in tqdm(text_data.items(),desc=\"Creating individual datasets\"):\n",
    "        df = pd.DataFrame({\n",
    "            'text': lines,\n",
    "            'index': list(range(len(lines)))\n",
    "        })\n",
    "        language_datasets[lang] = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Step 3: Create a merged dataset\n",
    "    # Start with English dataset as base\n",
    "    if 'eng' not in language_datasets:\n",
    "        raise ValueError(\"English dataset is required as the base for alignment\")\n",
    "    \n",
    "    # Get the number of examples in the English dataset\n",
    "    num_examples = len(language_datasets['eng'])\n",
    "    \n",
    "    # Verify all languages have the same number of examples\n",
    "    for lang, dataset in language_datasets.items():\n",
    "        if len(dataset) != num_examples:\n",
    "            print(f\"Warning: {lang} dataset has {len(dataset)} examples, but English has {num_examples}\")\n",
    "    \n",
    "    # Create a dictionary for the merged dataset\n",
    "    merged_data = {\n",
    "        'line_idx': list(range(num_examples)),\n",
    "        'eng': language_datasets['eng']['text']\n",
    "    }\n",
    "    \n",
    "    # Add other languages\n",
    "    for lang in languages:\n",
    "        if lang != 'eng' and lang in language_datasets:\n",
    "            if len(language_datasets[lang]) == num_examples:\n",
    "                merged_data[lang] = language_datasets[lang]['text']\n",
    "            else:\n",
    "                # Handle mismatched sizes by padding with empty strings\n",
    "                padded_texts = language_datasets[lang]['text'] + [''] * (num_examples - len(language_datasets[lang]))\n",
    "                merged_data[lang] = padded_texts[:num_examples]\n",
    "    \n",
    "    # Create the merged dataset\n",
    "    merged_dataset = Dataset.from_dict(merged_data)\n",
    "    \n",
    "    '''\n",
    "    # Add OpenSubtitles reference information if available\n",
    "    if ids_data:\n",
    "        print(\"Adding OpenSubtitles reference information...\")\n",
    "        # This could be expanded to add the mapping information to the dataset\n",
    "        # For now, we just note that it's available\n",
    "    \n",
    "    # Step 4: Load the .ids files (optional)\n",
    "    ids_data = {}\n",
    "    \n",
    "    # Discover all .ids files\n",
    "    ids_files = {}\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".ids\"):\n",
    "            lang_pair = file.split(\".\")[-2]  # Get the lang-eng part\n",
    "            ids_files[lang_pair] = os.path.join(data_dir, file)\n",
    "    \n",
    "    for lang_pair, file_path in ids_files.items():\n",
    "        ids_data[lang_pair] = load_ids_file(file_path)\n",
    "        print(f\"Loaded {lang_pair} file with {len(ids_data[lang_pair])} lines\")\n",
    "   ''' \n",
    "    # Create a dataset dictionary with train split\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': merged_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "def get_parallel_sentences(dataset, index, languages=None):\n",
    "    \"\"\"\n",
    "    Get parallel sentences for a specific example across all or specified languages.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset containing parallel sentences\n",
    "        index: The index of the example to retrieve\n",
    "        languages: Optional list of language codes to include (default: all available)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to sentences\n",
    "    \"\"\"\n",
    "    if index >= len(dataset) or index < 0:\n",
    "        raise ValueError(f\"Index {index} out of range (0-{len(dataset)-1})\")\n",
    "    \n",
    "    example = dataset[index]\n",
    "    \n",
    "    if languages is None:\n",
    "        # Get all available languages excluding non-text columns\n",
    "        languages = [col for col in example.keys() if col not in ['line_idx', 'index']]\n",
    "    \n",
    "    return {lang: example[lang] for lang in languages if lang in example}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding languages: 100%|██████████| 14/14 [00:00<?, ?it/s]\n",
      "Loading language files:  29%|██▊       | 2/7 [00:00<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eng file with 120600 lines\n",
      "Loaded arb file with 120600 lines\n",
      "Loaded apc file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading language files:  86%|████████▌ | 6/7 [00:00<00:00, 12.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded deu file with 120600 lines\n",
      "Loaded ell file with 120600 lines\n",
      "Loaded fra file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading language files: 100%|██████████| 7/7 [00:00<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spa file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating individual datasets: 100%|██████████| 7/7 [00:00<00:00,  9.06it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = \".\\\\..\\\\..\\\\Data\\\\UFAL Parallel Corpus of North Levantine 1.0\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_north_levantine_dataset(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line_idx': 0,\n",
       " 'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'دعونا نتحدّث عن الوقت الذي (قدّمت فيه (مالدوفا) لـ(رومانيا ،كعكة عيد ميلاد و (رومانيا) قالت أنّ طعمها جيّد حتّى و إن لم تكن كذلك',\n",
       " 'apc': 'خلونا نحكي عن الوقت اللي قدمت فيه مالدوفا لرومانيا، كعكة عيد ميلاد ورومانيا قالت إنو طيبة حتى لو ما كانت طيبة',\n",
       " 'deu': 'Reden wir darüber, als Moldavien Rumänien einen Geburtstagskuchen machte und Rumänien meinte, er wäre lecker, obwohl er das überhaupt nicht war.',\n",
       " 'ell': 'Ας μιλήσουμε για το όταν η Μολδαβία έκανε στη Ρουμανία μια τούρτα και η Ρουμανία είπε ότι είναι νόστιμη, αν και δεν ήταν.',\n",
       " 'fra': \"Parlons de l'époque Moldova a fait la Roumanie un gâteau d'anniversaire et de la Roumanie a déclaré qu'il avait bon goût même si elle n'a pas fait.\",\n",
       " 'spa': 'Hablemos de la vez que Moldavia le hizo a Rumanía un pastel de cumpleaños y Rumanía dijo que sabía bien aunque no era verdad.'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of examples: 120600\n",
      "Features: {'line_idx': Value(dtype='int64', id=None), 'eng': Value(dtype='string', id=None), 'arb': Value(dtype='string', id=None), 'apc': Value(dtype='string', id=None), 'deu': Value(dtype='string', id=None), 'ell': Value(dtype='string', id=None), 'fra': Value(dtype='string', id=None), 'spa': Value(dtype='string', id=None)}\n",
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 7138):\n",
      "  eng: Oh, my God, what is that smell?\n",
      "  arb: يا إلهي ما هذه الرائحة ؟\n",
      "  apc: يا ربي شو هالريحة؟\n",
      "  deu: Ich habe da immerhin 2 Jahre gelebt. Soll ich etwa so tun, als ob ich da nie gewesen wäre?\n",
      "  ell: Τι είναι αυτή η μυρωδιά\n",
      "  fra: Oh, mon Dieu, c'est quoi cette odeur ?\n",
      "  spa: Dios mío, ¿qué es ese olor?\n",
      "\n",
      "Example 2 (Index 36021):\n",
      "  eng: Darren, you don't owe these cops anything.\n",
      "  arb: دارين) أنت لا تدين للشرطة بأيّ شيء) .\n",
      "  apc: دارين أنت ما بتدين للشرطة بشي.\n",
      "  deu: Darren, du bist diesen Cops überhaupt nichts schuldig.\n",
      "  ell: Ντάρεν, δεν χρωστάς σε αυτούς τους μπάτσους τίποτα.\n",
      "  fra: Darren, tu ne dois rien à ces flics.\n",
      "  spa: Darren, no les debes nada a esos policías.\n",
      "\n",
      "Example 3 (Index 67721):\n",
      "  eng: What was Tesso looking for in those books he ate.\n",
      "  arb: ماذا كان يبحث (تاسو) في تلك الكتب التي اكلها\n",
      "  apc: عشو كان عم يدور تاسو بالكتب يلي أكلها\n",
      "  deu: Was hat Tesso in den Büchern gesucht, die er gegessen hat.\n",
      "  ell: Τι έψαχνε να βρει ο Τέσο στα βιβλία που έφαγε\n",
      "  fra: Que cherchait Tesso dans ce qu'il a mangés ?\n",
      "  spa: Lo que buscaba Tesso en esos bks que se comió.\n",
      "\n",
      "Example 4 (Index 58956):\n",
      "  eng: It turns out we have a common forefather.\n",
      "  arb: غير معقول نحن . ننتمى لعائلة واحدة\n",
      "  apc: مو معقول نحن. من عيلة واحدة\n",
      "  deu: Da haben wir also einen gemeinsamen Vorfahren.\n",
      "  ell: Αποδεικνύεται ότι έχουμε έναν κοινό προπάτορα.\n",
      "  fra: J'en conclus que nous avons une ascendance commune.\n",
      "  spa: Entonces, resulta que tenemos el mismo antepasado.\n",
      "\n",
      "Example 5 (Index 12077):\n",
      "  eng: Open the pod door, Hal.\n",
      "  arb: أفتح باب الكبسولة يا هال .\n",
      "  apc: افتح باب الكبسولة يا هال\n",
      "  deu: Gondeltür öffnen, Hal.\n",
      "  ell: Άνοιξε την πόρτα, Χαλ.\n",
      "  fra: Ouvre la porte.\n",
      "  spa: Abre la puerta de la cápsula, Hal.\n"
     ]
    }
   ],
   "source": [
    "# Print some information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of examples: {len(dataset['train'])}\")\n",
    "print(f\"Features: {dataset['train'].features}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(dataset['train'])), min(5, len(dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    parallel_sentences = get_parallel_sentences(dataset['train'], idx)\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    for lang, sentence in parallel_sentences.items():\n",
    "        print(f\"  {lang}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 102510\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 18090\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 102510/102510 [00:01<00:00, 82004.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 18090/18090 [00:00<00:00, 71924.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset locally\n",
    "dataset.save_to_disk(\"../../Data/UFAL Parallel Corpus of North Levantine 1.0/Processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 103/103 [00:08<00:00, 11.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:15<00:00, 15.62s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:01<00:00, 11.74ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "c:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Karim - Work\\.cache\\huggingface\\hub\\datasets--KHuss--UFAL_levantine. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/KHuss/UFAL_levantine/commit/a24bbab9bc0e167a1ae1e45d377808cbed6b8e6c', commit_message='Upload dataset', commit_description='', oid='a24bbab9bc0e167a1ae1e45d377808cbed6b8e6c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/KHuss/UFAL_levantine', endpoint='https://huggingface.co', repo_type='dataset', repo_id='KHuss/UFAL_levantine'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"KHuss/UFAL_levantine\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 102510\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 18090\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "in our case, this will involve transliteration, and ensuring that each row has the appropriate features, i.e target and source languages, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذهبت إلى المكتبة.\n",
      "*hbt <lY Almktbp.\n",
      "ذهبت إلى المكتبة.\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "sentence = \"ذهبت إلى المكتبة.\"\n",
    "print(sentence)\n",
    "\n",
    "ar2bw = CharMapper.builtin_mapper('ar2bw')\n",
    "bw2ar = CharMapper.builtin_mapper('bw2ar')\n",
    "\n",
    "sent_bw = ar2bw(sentence)\n",
    "sent_ar = bw2ar(sent_bw)\n",
    "print(sent_bw)\n",
    "print(sent_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    return {\"eng\":row[\"eng\"], \"arb\": [ar2bw(ar) for ar in row[\"arb\"]] if isinstance(row[\"arb\"], list) else ar2bw(row[\"arb\"])}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120600/120600 [00:01<00:00, 92329.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test = dataset.map(process_row,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'dEwnA ntHd~v En Alwqt Al*y (qd~mt fyh (mAldwfA) l_(rwmAnyA ،kEkp Eyd mylAd w (rwmAnyA) qAlt >n~ TEmhA jy~d Ht~Y w <n lm tkn k*lk'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    comp = [row[\"eng\"],ar2bw(row[\"arb\"])] if not isinstance(row[\"arb\"], list) else row[\"eng\"].extend([ar2bw(ar) for ar in row[\"arb\"]])  \n",
    "    promp = [\"\"]*len(comp)\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    return {\"prompt\":promp,\"completion\":comp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/102510 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32mc:\\users\\karim - work\\appdata\\local\\temp\\ipykernel_49892\\99433667.py\u001b[39m(\u001b[92m4\u001b[39m)\u001b[36mformat_fn\u001b[39m\u001b[34m()\u001b[39m\n",
      "\n",
      "{'line_idx': 73903, 'eng': \"That farmer's dream is all some people have.\", 'arb': 'وحُلم المزارع هذا هوَ كلّ ما لدى بعض الناس', 'apc': 'حلم المزارع هاد كلّ يلي عند بعض الناس.', 'deu': 'Dieser Traum eines Farmers ist für viele alles, was sie haben.', 'ell': 'To όvειρo τoυ αγρότη είναι τo μόvo πoυ έχoυv κάπoιoι.', 'fra': 'Ce rêve représente tout pour certains.', 'spa': 'Hay personas que sólo tienen ese sueño de granjero.'}\n",
      "*** NameError: name 'comp' is not defined\n",
      "*** SyntaxError: unmatched ']'\n",
      "[\"That farmer's dream is all some people have.\", 'wHulm AlmzArE h*A hwa kl~ mA ldY bED AlnAs']\n",
      "False\n",
      "*** NameError: name 'vomp' is not defined\n",
      "[\"That farmer's dream is all some people have.\", 'wHulm AlmzArE h*A hwa kl~ mA ldY bED AlnAs']\n",
      "['', '']\n"
     ]
    }
   ],
   "source": [
    "dataset_test_2 = dataset.map(format_fn,remove_columns=dataset[\"train\"].column_names)#,batched=True,batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
