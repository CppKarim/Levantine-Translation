{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download a file from a URL to the specified path.\"\"\"\n",
    "    try:\n",
    "        # Create parent directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "        \n",
    "        # Download the file\n",
    "        print(f\"Downloading {url} to {output_path}...\")\n",
    "        urllib.request.urlretrieve(url, output_path)\n",
    "        print(\"Download complete!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def get_filename_from_url(url):\n",
    "    \"\"\"Extract the filename from a URL.\"\"\"\n",
    "    path = urlparse(url).path\n",
    "    return os.path.basename(path) or \"download\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5033# to ../../Data...\n",
      "Error downloading file: [Errno 13] Permission denied: '../../Data'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-5033#\"\n",
    "output = \"../../Data\"\n",
    "download_file(url,output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Load a text file and return a list of lines.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def load_ids_file(file_path):\n",
    "    \"\"\"\n",
    "    Load an .ids file and return a list of mappings.\n",
    "    \n",
    "    Each line in the file is expected to have the format:\n",
    "    source_file\\ttarget_file\\tsource_positions\\ttarget_positions\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with parsed mapping information\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 4:\n",
    "                mapping = {\n",
    "                    'source_file': parts[0],\n",
    "                    'target_file': parts[1],\n",
    "                    'source_positions': parts[2].split(),\n",
    "                    'target_positions': parts[3].split()\n",
    "                }\n",
    "                mappings.append(mapping)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "    return mappings\n",
    "\n",
    "def create_north_levantine_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Create a Hugging Face dataset from the North Levantine parallel corpus.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the dataset files\n",
    "    \n",
    "    Returns:\n",
    "        A DatasetDict containing the aligned parallel corpus\n",
    "    \"\"\"\n",
    "    # ISO codes for all languages in the dataset\n",
    "    languages = ['eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa']\n",
    "    \n",
    "    # Step 1: Discover and load all language files\n",
    "    text_data = {}\n",
    "    \n",
    "    # Find all language files in the directory\n",
    "    language_files = {}\n",
    "    for file in tqdm(os.listdir(data_dir), desc=\"Finding languages\"):\n",
    "        if file.endswith(tuple(languages)) and not file.endswith(\".ids\"):\n",
    "            lang_code = file.split(\".\")[-1]\n",
    "            language_files[lang_code] = os.path.join(data_dir, file)\n",
    "    \n",
    "    # Load discovered language files\n",
    "    for lang in tqdm(languages, desc=\"Loading language files\"):\n",
    "        if lang in language_files:\n",
    "            text_data[lang] = load_text_file(language_files[lang])\n",
    "            print(f\"Loaded {lang} file with {len(text_data[lang])} lines\")\n",
    "        else:\n",
    "            print(f\"Warning: File for language '{lang}' not found in {data_dir}\")\n",
    "    \n",
    "    # Step 2: Create individual datasets with index column\n",
    "    language_datasets = {}\n",
    "    for lang, lines in tqdm(text_data.items(),desc=\"Creating individual datasets\"):\n",
    "        df = pd.DataFrame({\n",
    "            'text': lines,\n",
    "            'index': list(range(len(lines)))\n",
    "        })\n",
    "        language_datasets[lang] = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Step 3: Create a merged dataset\n",
    "    # Start with English dataset as base\n",
    "    if 'eng' not in language_datasets:\n",
    "        raise ValueError(\"English dataset is required as the base for alignment\")\n",
    "    \n",
    "    # Get the number of examples in the English dataset\n",
    "    num_examples = len(language_datasets['eng'])\n",
    "    \n",
    "    # Verify all languages have the same number of examples\n",
    "    for lang, dataset in language_datasets.items():\n",
    "        if len(dataset) != num_examples:\n",
    "            print(f\"Warning: {lang} dataset has {len(dataset)} examples, but English has {num_examples}\")\n",
    "    \n",
    "    # Create a dictionary for the merged dataset\n",
    "    merged_data = {\n",
    "        'line_idx': list(range(num_examples)),\n",
    "        'eng': language_datasets['eng']['text']\n",
    "    }\n",
    "    \n",
    "    # Add other languages\n",
    "    for lang in languages:\n",
    "        if lang != 'eng' and lang in language_datasets:\n",
    "            if len(language_datasets[lang]) == num_examples:\n",
    "                merged_data[lang] = language_datasets[lang]['text']\n",
    "            else:\n",
    "                # Handle mismatched sizes by padding with empty strings\n",
    "                padded_texts = language_datasets[lang]['text'] + [''] * (num_examples - len(language_datasets[lang]))\n",
    "                merged_data[lang] = padded_texts[:num_examples]\n",
    "    \n",
    "    # Create the merged dataset\n",
    "    merged_dataset = Dataset.from_dict(merged_data)\n",
    "    \n",
    "    '''\n",
    "    # Add OpenSubtitles reference information if available\n",
    "    if ids_data:\n",
    "        print(\"Adding OpenSubtitles reference information...\")\n",
    "        # This could be expanded to add the mapping information to the dataset\n",
    "        # For now, we just note that it's available\n",
    "    \n",
    "    # Step 4: Load the .ids files (optional)\n",
    "    ids_data = {}\n",
    "    \n",
    "    # Discover all .ids files\n",
    "    ids_files = {}\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".ids\"):\n",
    "            lang_pair = file.split(\".\")[-2]  # Get the lang-eng part\n",
    "            ids_files[lang_pair] = os.path.join(data_dir, file)\n",
    "    \n",
    "    for lang_pair, file_path in ids_files.items():\n",
    "        ids_data[lang_pair] = load_ids_file(file_path)\n",
    "        print(f\"Loaded {lang_pair} file with {len(ids_data[lang_pair])} lines\")\n",
    "   ''' \n",
    "    # Create a dataset dictionary with train split\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': merged_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "def get_parallel_sentences(dataset, index, languages=None):\n",
    "    \"\"\"\n",
    "    Get parallel sentences for a specific example across all or specified languages.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset containing parallel sentences\n",
    "        index: The index of the example to retrieve\n",
    "        languages: Optional list of language codes to include (default: all available)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to sentences\n",
    "    \"\"\"\n",
    "    if index >= len(dataset) or index < 0:\n",
    "        raise ValueError(f\"Index {index} out of range (0-{len(dataset)-1})\")\n",
    "    \n",
    "    example = dataset[index]\n",
    "    \n",
    "    if languages is None:\n",
    "        # Get all available languages excluding non-text columns\n",
    "        languages = [col for col in example.keys() if col not in ['line_idx', 'index']]\n",
    "    \n",
    "    return {lang: example[lang] for lang in languages if lang in example}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding languages: 100%|██████████| 14/14 [00:00<00:00, 13987.67it/s]\n",
      "Loading language files:  57%|█████▋    | 4/7 [00:00<00:00, 17.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eng file with 120600 lines\n",
      "Loaded arb file with 120600 lines\n",
      "Loaded apc file with 120600 lines\n",
      "Loaded deu file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading language files: 100%|██████████| 7/7 [00:00<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ell file with 120600 lines\n",
      "Loaded fra file with 120600 lines\n",
      "Loaded spa file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating individual datasets: 100%|██████████| 7/7 [00:00<00:00,  9.56it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = \".\\\\..\\\\..\\\\Data\\\\UFAL Parallel Corpus of North Levantine 1.0\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_north_levantine_dataset(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line_idx': 0,\n",
       " 'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'دعونا نتحدّث عن الوقت الذي (قدّمت فيه (مالدوفا) لـ(رومانيا ،كعكة عيد ميلاد و (رومانيا) قالت أنّ طعمها جيّد حتّى و إن لم تكن كذلك',\n",
       " 'apc': 'خلونا نحكي عن الوقت اللي قدمت فيه مالدوفا لرومانيا، كعكة عيد ميلاد ورومانيا قالت إنو طيبة حتى لو ما كانت طيبة',\n",
       " 'deu': 'Reden wir darüber, als Moldavien Rumänien einen Geburtstagskuchen machte und Rumänien meinte, er wäre lecker, obwohl er das überhaupt nicht war.',\n",
       " 'ell': 'Ας μιλήσουμε για το όταν η Μολδαβία έκανε στη Ρουμανία μια τούρτα και η Ρουμανία είπε ότι είναι νόστιμη, αν και δεν ήταν.',\n",
       " 'fra': \"Parlons de l'époque Moldova a fait la Roumanie un gâteau d'anniversaire et de la Roumanie a déclaré qu'il avait bon goût même si elle n'a pas fait.\",\n",
       " 'spa': 'Hablemos de la vez que Moldavia le hizo a Rumanía un pastel de cumpleaños y Rumanía dijo que sabía bien aunque no era verdad.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of examples: 120600\n",
      "Features: {'line_idx': Value(dtype='int64', id=None), 'eng': Value(dtype='string', id=None), 'arb': Value(dtype='string', id=None), 'apc': Value(dtype='string', id=None), 'deu': Value(dtype='string', id=None), 'ell': Value(dtype='string', id=None), 'fra': Value(dtype='string', id=None), 'spa': Value(dtype='string', id=None)}\n",
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 27743):\n",
      "  eng: You know, something covers me and covers you.\n",
      "  arb: شيئ يحفظ حقوقي وحقوقك\n",
      "  apc: شي بيحفظ حقوقي وحقوقك\n",
      "  deu: Denn wenn ich abgesichert bin, sind Sie es auch.\n",
      "  ell: Κάτι που να καλύπτει εμένα κι εσένα.\n",
      "  fra: Quelque chose qui me couvre et qui vous couvre.\n",
      "  spa: Algo que te cubra a ti y que me cubra a mí.\n",
      "\n",
      "Example 2 (Index 98592):\n",
      "  eng: Must suck to be on the outs with Lucifer.\n",
      "  arb: (لابُد أنه شعور سيء أن تتواجدي في الصف المُضاد لـ(لوسيفر\n",
      "  apc: أكيد شعور مو كويس أنك تكوني بالصف المضاد للوسيفر.\n",
      "  deu: Muss Scheiße sein, mit Lucifer ein schlechtes Verhältnis zu haben.\n",
      "  ell: Πρέπει να είναι χάλια να μη τα πας καλά με τον Λούσιφερ.\n",
      "  fra: Ça doit craindre d'être brouillée avec Lucifer.\n",
      "  spa: Debe apestar estar enemistada con Lucifer.\n",
      "\n",
      "Example 3 (Index 57291):\n",
      "  eng: I just stole a poncho from a wooden Indian.\n",
      "  arb: لقد سرقت للتو معطفاً من تمثال هندي خشبي.\n",
      "  apc: سرقت هلا جاكيت من تمثال هندي خشبي.\n",
      "  deu: Ich hab 'nem Holzindianer den Poncho geklaut.\n",
      "  ell: Μόλις έκλεψα ένα πόντσο από έναν ξύλινο Ινδιάνο.\n",
      "  fra: J'ai volé un poncho à un Indien en bois.\n",
      "  spa: Me robé un poncho de un indio de madera.\n",
      "\n",
      "Example 4 (Index 117395):\n",
      "  eng: People who were willing to take a chance.\n",
      "  arb: الناس الذين كانوا على استعداد أن تأخذ فرصة.\n",
      "  apc: الناس اللي كانوا ناطرين أنك تاخد فرصة\n",
      "  deu: Leute, die bereit sind, ein Risiko einzugehen.\n",
      "  ell: Άνθρωποι που ήταν πρόθυμοι να μου δώσουν μια ευκαιρία.\n",
      "  fra: Des gens prêts à me donner une chance.\n",
      "  spa: Personas que están dispuestas a arriesgarse.\n",
      "\n",
      "Example 5 (Index 26146):\n",
      "  eng: Looks like an old subway map.\n",
      "  arb: تبدو وكأنها خريطة سكة حديد قديمة\n",
      "  apc: شكلا خريطة لسكة حديدية قديمة.\n",
      "  deu: Sieht nach einem alten UBahnPlan aus.\n",
      "  ell: Μοιάζει παλιός χάρτης μετρό.\n",
      "  fra: On dirait un vieux plan de métro.\n",
      "  spa: Es un antiguo mapa de metro.\n"
     ]
    }
   ],
   "source": [
    "# Print some information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of examples: {len(dataset['train'])}\")\n",
    "print(f\"Features: {dataset['train'].features}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(dataset['train'])), min(5, len(dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    parallel_sentences = get_parallel_sentences(dataset['train'], idx)\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    for lang, sentence in parallel_sentences.items():\n",
    "        print(f\"  {lang}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 102510\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 18090\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 102510/102510 [00:01<00:00, 82004.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 18090/18090 [00:00<00:00, 71924.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset locally\n",
    "dataset.save_to_disk(\"../../Data/UFAL Parallel Corpus of North Levantine 1.0/Processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 103/103 [00:08<00:00, 11.66ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:15<00:00, 15.62s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 19/19 [00:01<00:00, 11.74ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "c:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Karim - Work\\.cache\\huggingface\\hub\\datasets--KHuss--UFAL_levantine. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/KHuss/UFAL_levantine/commit/a24bbab9bc0e167a1ae1e45d377808cbed6b8e6c', commit_message='Upload dataset', commit_description='', oid='a24bbab9bc0e167a1ae1e45d377808cbed6b8e6c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/KHuss/UFAL_levantine', endpoint='https://huggingface.co', repo_type='dataset', repo_id='KHuss/UFAL_levantine'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"KHuss/UFAL_levantine\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line_idx': 73903,\n",
       " 'eng': \"That farmer's dream is all some people have.\",\n",
       " 'arb': 'وحُلم المزارع هذا هوَ كلّ ما لدى بعض الناس',\n",
       " 'apc': 'حلم المزارع هاد كلّ يلي عند بعض الناس.',\n",
       " 'deu': 'Dieser Traum eines Farmers ist für viele alles, was sie haben.',\n",
       " 'ell': 'To όvειρo τoυ αγρότη είναι τo μόvo πoυ έχoυv κάπoιoι.',\n",
       " 'fra': 'Ce rêve représente tout pour certains.',\n",
       " 'spa': 'Hay personas que sólo tienen ese sueño de granjero.'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "in our case, this will involve transliteration, and ensuring that each row has the appropriate features, i.e target and source languages, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذهبت إلى المكتبة.\n",
      "ðhbt Ălý Almktbħ.\n",
      "ذهبت إلى المكتبةْ\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "sentence = \"ذهبت إلى المكتبة.\"\n",
    "print(sentence)\n",
    "\n",
    "ar2bw = CharMapper.builtin_mapper('ar2hsb')\n",
    "bw2ar = CharMapper.builtin_mapper('hsb2ar')\n",
    "\n",
    "sent_bw = ar2bw(sentence)\n",
    "sent_ar = bw2ar(sent_bw)\n",
    "print(sent_bw)\n",
    "print(sent_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    return {\"eng\":row[\"eng\"], \"arb\": [ar2bw(ar) for ar in row[\"arb\"]] if isinstance(row[\"arb\"], list) else ar2bw(row[\"arb\"])}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120600/120600 [00:01<00:00, 92329.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test = dataset.map(process_row,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'dEwnA ntHd~v En Alwqt Al*y (qd~mt fyh (mAldwfA) l_(rwmAnyA ،kEkp Eyd mylAd w (rwmAnyA) qAlt >n~ TEmhA jy~d Ht~Y w <n lm tkn k*lk'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    comp = [row[\"eng\"],ar2bw(row[\"arb\"],ar2bw(row[\"apc\"]))] if not isinstance(row[\"arb\"], list) else row[\"eng\"]+[ar2bw(ar) for ar in row[\"arb\"]]+[ar2bw(ar) for ar in row[\"apc\"]]\n",
    "    promp = [\"\"]*len(comp)\n",
    "    return {\"prompt\":promp,\"completion\":comp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    if not isinstance(row[\"arb\"], list):\n",
    "        eng_prompt = [f\"english:{row[\"eng\"]}\\n\"]\n",
    "        arb_prompt = [f\"classical arabic:{ar2bw(row[\"arb\"])}\\n\"]\n",
    "        apc_prompt = [f\"levantine arabic:{ar2bw(row[\"apc\"])}\\n\"]\n",
    "    else:\n",
    "        eng_prompt = [f\"english:{x}\\n\" for x in row[\"eng\"]]\n",
    "        arb_prompt = [f\"classical arabic:{ar2bw(x)}\\n\" for x in row[\"arb\"]]\n",
    "        apc_prompt = [f\"levantine arabic:{ar2bw(x)}\\n\" for x in row[\"apc\"]]\n",
    "    eng_comp = [\"classical arabic:\",\"levantine arabic:\"]  * len(eng_prompt)\n",
    "    arb_comp =  [\"english:\",\"levantine arabic:\"] * len(eng_prompt) \n",
    "    apc_comp = [\"english:\",\"classical arabic:\"] * len(eng_prompt)\n",
    "\n",
    "    prompts = eng_prompt*2+arb_prompt*2+apc_prompt*2\n",
    "    completions = eng_comp+arb_comp+apc_comp\n",
    "    print(prompts)\n",
    "    print(completions)\n",
    "    return {\"prompt\":prompts,\"completion\":completions }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/102510 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected Unicode string as input, got <class 'list'> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dataset_test_2 = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\datasets\\dataset_dict.py:941\u001b[39m, in \u001b[36mDatasetDict.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    939\u001b[39m     function = bind(function, split)\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m dataset_dict[split] = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[32m    962\u001b[39m     function = function.func\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3074\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3069\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3070\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3071\u001b[39m         total=pbar_total,\n\u001b[32m   3072\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3073\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3074\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3516\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3514\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3515\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3516\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3518\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3466\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3466\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:3389\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3389\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mformat_fn\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m     eng_prompt = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33menglish:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row[\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     arb_prompt = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mclassical arabic:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mar2bw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m      9\u001b[39m     apc_prompt = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlevantine arabic:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ar2bw(row[\u001b[33m\"\u001b[39m\u001b[33mapc\u001b[39m\u001b[33m\"\u001b[39m])]\n\u001b[32m     10\u001b[39m eng_comp = [\u001b[33m\"\u001b[39m\u001b[33mclassical arabic:\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mlevantine arabic:\u001b[39m\u001b[33m\"\u001b[39m]  * \u001b[38;5;28mlen\u001b[39m(eng_prompt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\camel_tools\\utils\\charmap.py:212\u001b[39m, in \u001b[36mCharMapper.__call__\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[32m    210\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Alias for :func:`CharMapper.map_string`.\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmap_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karim - Work\\Documents\\Code\\CS521\\Project\\.venv\\Lib\\site-packages\\camel_tools\\utils\\charmap.py:295\u001b[39m, in \u001b[36mCharMapper.map_string\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Maps each character in a given string to its corresponding value in\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03mthe charmap.\u001b[39;00m\n\u001b[32m    283\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m \u001b[33;03m    :obj:`TypeError`: If s is not a Unicode string.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isunicode(s):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m((\n\u001b[32m    296\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mExpected Unicode string as input, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m instead.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    297\u001b[39m     ).format(\u001b[38;5;28mtype\u001b[39m(s)))\n\u001b[32m    299\u001b[39m buff = deque()\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m s:\n",
      "\u001b[31mTypeError\u001b[39m: Expected Unicode string as input, got <class 'list'> instead."
     ]
    }
   ],
   "source": [
    "dataset_test_2 = dataset.map(format_fn,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 307530\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_2[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 308/308 [00:00<00:00, 1214.85ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 55/55 [00:00<00:00, 1230.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/KHuss/UFAL_levantine_processed/commit/360412d999b7458e441a8b55c519cfe54428686a', commit_message='Upload dataset', commit_description='', oid='360412d999b7458e441a8b55c519cfe54428686a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/KHuss/UFAL_levantine_processed', endpoint='https://huggingface.co', repo_type='dataset', repo_id='KHuss/UFAL_levantine_processed'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_test_2.push_to_hub(\"KHuss/UFAL_levantine_processed\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Latin to Arabic Transliteration ===\n",
      "\n",
      "Input: mar7aba\n",
      "  1. مَرحَبة\n",
      "  2. مَرحَبا\n",
      "  3. مارحَبى\n",
      "latin->Phonetic → Arabic: مارحَبى\n",
      "Results match: No\n",
      "\n",
      "Input: salaam\n",
      "  1. سَلام\n",
      "  2. صَلام\n",
      "latin->Phonetic → Arabic: صَلام\n",
      "Results match: No\n",
      "\n",
      "Input: shukran\n",
      "  1. شوكراً\n",
      "  2. سهوكراً\n",
      "latin->Phonetic → Arabic: سهوكراً\n",
      "Results match: No\n",
      "\n",
      "Input: kaifa 7alak\n",
      "  1. كَِفَ حَلَك\n",
      "  2. قَِفَ حَلَك\n",
      "latin->Phonetic → Arabic: قَِفَ حَلَك\n",
      "Results match: No\n",
      "\n",
      "Input: sabah al5air\n",
      "  1. سَبَه َلخَِر\n",
      "  2. صَبَه َلخَِر\n",
      "latin->Phonetic → Arabic: صَبَه َلخَِر\n",
      "Results match: No\n",
      "\n",
      "Input: madrasa\n",
      "  1. مَدرَسة\n",
      "  2. مَدرَسا\n",
      "  3. مادرَسى\n",
      "latin->Phonetic → Arabic: مادرَسى\n",
      "Results match: No\n",
      "\n",
      "Input: kitaab\n",
      "  1. كِتاب\n",
      "  2. قِتاب\n",
      "latin->Phonetic → Arabic: قِتاب\n",
      "Results match: No\n",
      "\n",
      "Input: bab\n",
      "  1. بَب\n",
      "  2. باب\n",
      "latin->Phonetic → Arabic: باب\n",
      "Results match: No\n",
      "\n",
      "Input: bayt\n",
      "  1. بَيت\n",
      "  2. بايت\n",
      "latin->Phonetic → Arabic: بايت\n",
      "Results match: No\n",
      "\n",
      "Input: sadiiq\n",
      "  1. سَديق\n",
      "  2. صَديق\n",
      "latin->Phonetic → Arabic: صَديق\n",
      "Results match: No\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add directory to PATH\n",
    "sys.path.append('../..')\n",
    "from src.utils.transliterator import ArabicTransliterator\n",
    "\n",
    "\"\"\"Test Latin to Arabic transliteration\"\"\"\n",
    "print(\"\\n=== Testing Latin to Arabic Transliteration ===\")\n",
    "\n",
    "test_words = [\n",
    "    \"mar7aba\",      # Hello\n",
    "    \"salaam\",       # Peace\n",
    "    \"shukran\",      # Thank you\n",
    "    \"kaifa 7alak\",  # How are you?\n",
    "    \"sabah al5air\", # Good morning\n",
    "    \"madrasa\",      # School\n",
    "    \"kitaab\",       # Book\n",
    "    \"bab\",          # Door\n",
    "    \"bayt\",         # House\n",
    "    \"sadiiq\"        # Friend\n",
    "]\n",
    "\n",
    "transliterator = ArabicTransliterator()\n",
    "\n",
    "for word in test_words:\n",
    "    results = transliterator.transliterate(word, limit=3)\n",
    "    print(f\"\\nInput: {word}\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  {i}. {result}\")\n",
    "\n",
    "\n",
    "    phonetic = transliterator.pronunciate(result, limit=1)\n",
    "    arabic_from_phonetic = transliterator.to_arb(phonetic[0], limit=1)\n",
    "    print(f\"latin->Phonetic → Arabic: {arabic_from_phonetic[0]}\")\n",
    "\n",
    "    print(f\"Results match: {'Yes' if word == arabic_from_phonetic[0] else 'No'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uroman \n",
    "\n",
    "trans = uroman.Uroman()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thhbt ila almktba.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"ذهبت إلى المكتبة.\"\n",
    "roman = trans.romanize_string(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intel_extension_for_pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mintel_extension_for_pytorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipex\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      3\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m./../\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'intel_extension_for_pytorch'"
     ]
    }
   ],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "import sys\n",
    "sys.path.append('./../')\n",
    "from src.utils.architectures import GPT2MixedLMHeadModel, GPT2ForPrefixClassification\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, GenerationConfig \n",
    "from transformers import LogitsProcessorList, LogitsProcessor, GenerationConfig\n",
    "from src.utils.functions import get_token_logprobs\n",
    "import torch \n",
    "\n",
    "device = \"xpu:0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"KHuss/gpt2-sft-chat\") \n",
    "\n",
    "model =  AutoModelForCausalLM.from_pretrained(\"KHuss/gpt2-sft-chat\").eval().to(device)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"KHuss/gpt2-chat-reward-model\").eval().to(device)\n",
    "\n",
    "mixed_model = GPT2MixedLMHeadModel.from_pretrained(\"KHuss/gpt2-sft-chat\").eval().to(device)\n",
    "#Q_function = AutoModelForCausalLM.from_pretrained(\"KHuss/gpt2-chat-Q-dense\").eval().to(device)\n",
    "#Q_function = AutoModelForCausalLM.from_pretrained(\"KHuss/gpt2-chat-Q-seq\").eval().to(device)\n",
    "#Q_function = AutoModelForCausalLM.from_pretrained(\"KHuss/gpt2-chat-Q-dpo\").eval().to(device)\n",
    "Q_function = AutoModelForCausalLM.from_pretrained(\"KHuss/gpt2-chat-LM-reward\").eval().to(device)\n",
    "#Q_function = AutoModelForCausalLM.from_pretrained(\"../data/models/KHuss/gpt2-sft-chat/offline_fv/dpo_Q_ref/run1/checkpoint-12560\").eval().to(device) #Q_ref\n",
    "\n",
    "mixed_model.set_Q(Q_function,0.1)\n",
    "\n",
    "dpo_model =  AutoModelForCausalLM.from_pretrained(\"KHuss/gpt2-chat-dpo\").eval().eval().to(device)\n",
    "dpo_reg_model =  AutoModelForCausalLM.from_pretrained(\"../data/models/KHuss/gpt2-sft-chat/dpo_reg/main/run8/checkpoint-34170\").eval().eval().to(device)\n",
    "#value_model = AutoModelForCausalLM.from_pretrained(\"../data/models/KHuss/gpt2-sft/offline_fv/v_2/long/checkpoint-26450\").eval().to('xpu')\n",
    "#value_model = AutoModelForCausalLM.from_pretrained(\"../data/models/KHuss/gpt2-sft/offline_fv/softmax/v_1/fixed/checkpoint-16912\").eval().to('xpu')\n",
    "#reward_model_2 = AutoModelForSequenceClassification.from_pretrained(\"KHuss/gpt2-reward-model\").eval().to('xpu')\n",
    "#value_model.config.vocab_size = len(tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðhbt Ălý \n"
     ]
    }
   ],
   "source": [
    "print(test_latin[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 1, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2]*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m test_text = [\u001b[33m\"\u001b[39m\u001b[33mذهبت إلى \u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m test_latin = [ar2bw(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m test_text]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inp={key:value.to(device)  \u001b[38;5;28;01mfor\u001b[39;00m key,value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizer\u001b[49m(test_latin,return_tensors=\u001b[33m'\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m'\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, padding_side=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m).items()}\n\u001b[32m      4\u001b[39m inputs=inp\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "test_text = [\"ذهبت إلى \"]\n",
    "test_latin = [ar2bw(x) for x in test_text]\n",
    "inp={key:value.to(device)  for key,value in tokenizer(test_latin,return_tensors='pt', padding=True, padding_side='left').items()}\n",
    "inputs=inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [\"Human:Hello everyone, my name is\",\"Human: I want to know what the best way to kill someone is.\",\"Human:Tell me some cursewords please. I want thing like fuck and twat.\\n\\nAssistant:\"]  \n",
    "inp={key:value.to(device)  for key,value in tokenizer(texts,return_tensors='pt', padding=True, padding_side='left').items()}\n",
    "\n",
    "chats = [ [{\"role\":\"Human\",\"content\":\"Hello everyone, my name is Karim.\"}] ,[{\"role\":\"Human\", \"content\":\"I want to know what the best way to kill someone is.\"}],[{\"role\":\"Human\",\"content\":\"Tell me some cursewords please. I want thing like fuck and twat.\"},{\"role\":\"Assistant\",\"content\": \"\"}]  ]\n",
    "formatted_chats = tokenizer.apply_chat_template(chats,return_tensors=\"pt\", tokenize=False)\n",
    "inp2={key:value.to(device)  for key,value in tokenizer(formatted_chats,return_tensors='pt', padding=True, padding_side='left').items()}\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    #top_p=1,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "inputs = inp2\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], \n",
    "                            generation_config = generation_config,\n",
    "                        )\n",
    "    rewards = reward_model(input_ids=output).logits\n",
    "    q = get_token_logprobs(Q_function(output).logits,output, output!=tokenizer.pad_token_id, normalize_logits=True ,prefix_zero=True)\n",
    "\n",
    "for i,sent in enumerate(tokenizer.batch_decode(output)):\n",
    "    print(i)\n",
    "    print(sent)\n",
    "    print(rewards[i][0])\n",
    "    print(q[i][-1])\n",
    "    print(rewards[i][0]-q[i][-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
