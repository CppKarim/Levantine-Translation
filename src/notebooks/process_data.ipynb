{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline\n",
    "This notebook is an interactive version of the data processing files, and also allows to explore the dataset interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "This script assume the dataset has already been downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sentences are placed in different files in the source, we haev to match each row from different language files using ids files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Load a text file and return a list of lines.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def load_ids_file(file_path):\n",
    "    \"\"\"\n",
    "    Load an .ids file and return a list of mappings.\n",
    "    \n",
    "    Each line in the file is expected to have the format:\n",
    "    source_file\\ttarget_file\\tsource_positions\\ttarget_positions\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with parsed mapping information\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 4:\n",
    "                mapping = {\n",
    "                    'source_file': parts[0],\n",
    "                    'target_file': parts[1],\n",
    "                    'source_positions': parts[2].split(),\n",
    "                    'target_positions': parts[3].split()\n",
    "                }\n",
    "                mappings.append(mapping)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "    return mappings\n",
    "\n",
    "def create_north_levantine_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Create a Hugging Face dataset from the North Levantine parallel corpus.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the dataset files\n",
    "    \n",
    "    Returns:\n",
    "        A DatasetDict containing the aligned parallel corpus\n",
    "    \"\"\"\n",
    "    # ISO codes for all languages in the dataset\n",
    "    languages = ['eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa']\n",
    "    \n",
    "    # Step 1: Discover and load all language files\n",
    "    text_data = {}\n",
    "    \n",
    "    # Find all language files in the directory\n",
    "    language_files = {}\n",
    "    for file in tqdm(os.listdir(data_dir), desc=\"Finding languages\"):\n",
    "        if file.endswith(tuple(languages)) and not file.endswith(\".ids\"):\n",
    "            lang_code = file.split(\".\")[-1]\n",
    "            language_files[lang_code] = os.path.join(data_dir, file)\n",
    "    \n",
    "    # Load discovered language files\n",
    "    for lang in tqdm(languages, desc=\"Loading language files\"):\n",
    "        if lang in language_files:\n",
    "            text_data[lang] = load_text_file(language_files[lang])\n",
    "            print(f\"Loaded {lang} file with {len(text_data[lang])} lines\")\n",
    "        else:\n",
    "            print(f\"Warning: File for language '{lang}' not found in {data_dir}\")\n",
    "    \n",
    "    # Step 2: Create individual datasets with index column\n",
    "    language_datasets = {}\n",
    "    for lang, lines in tqdm(text_data.items(),desc=\"Creating individual datasets\"):\n",
    "        df = pd.DataFrame({\n",
    "            'text': lines,\n",
    "            'index': list(range(len(lines)))\n",
    "        })\n",
    "        language_datasets[lang] = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Step 3: Create a merged dataset\n",
    "    # Start with English dataset as base\n",
    "    if 'eng' not in language_datasets:\n",
    "        raise ValueError(\"English dataset is required as the base for alignment\")\n",
    "    \n",
    "    # Get the number of examples in the English dataset\n",
    "    num_examples = len(language_datasets['eng'])\n",
    "    \n",
    "    # Verify all languages have the same number of examples\n",
    "    for lang, dataset in language_datasets.items():\n",
    "        if len(dataset) != num_examples:\n",
    "            print(f\"Warning: {lang} dataset has {len(dataset)} examples, but English has {num_examples}\")\n",
    "    \n",
    "    # Create a dictionary for the merged dataset\n",
    "    merged_data = {\n",
    "        'line_idx': list(range(num_examples)),\n",
    "        'eng': language_datasets['eng']['text']\n",
    "    }\n",
    "    \n",
    "    # Add other languages\n",
    "    for lang in languages:\n",
    "        if lang != 'eng' and lang in language_datasets:\n",
    "            if len(language_datasets[lang]) == num_examples:\n",
    "                merged_data[lang] = language_datasets[lang]['text']\n",
    "            else:\n",
    "                # Handle mismatched sizes by padding with empty strings\n",
    "                padded_texts = language_datasets[lang]['text'] + [''] * (num_examples - len(language_datasets[lang]))\n",
    "                merged_data[lang] = padded_texts[:num_examples]\n",
    "    \n",
    "    # Create the merged dataset\n",
    "    merged_dataset = Dataset.from_dict(merged_data)\n",
    "    \n",
    "    # Create a dataset dictionary with train split\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': merged_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "def get_parallel_sentences(dataset, index, languages=None):\n",
    "    \"\"\"\n",
    "    Get parallel sentences for a specific example across all or specified languages.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset containing parallel sentences\n",
    "        index: The index of the example to retrieve\n",
    "        languages: Optional list of language codes to include (default: all available)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to sentences\n",
    "    \"\"\"\n",
    "    if index >= len(dataset) or index < 0:\n",
    "        raise ValueError(f\"Index {index} out of range (0-{len(dataset)-1})\")\n",
    "    \n",
    "    example = dataset[index]\n",
    "    \n",
    "    if languages is None:\n",
    "        # Get all available languages excluding non-text columns\n",
    "        languages = [col for col in example.keys() if col not in ['line_idx', 'index']]\n",
    "    \n",
    "    return {lang: example[lang] for lang in languages if lang in example}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding languages: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "Loading language files:  43%|████▎     | 3/7 [00:00<00:00, 19.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eng file with 120600 lines\n",
      "Loaded arb file with 120600 lines\n",
      "Loaded apc file with 120600 lines\n",
      "Loaded deu file with 120600 lines\n",
      "Loaded ell file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading language files: 100%|██████████| 7/7 [00:00<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fra file with 120600 lines\n",
      "Loaded spa file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating individual datasets: 100%|██████████| 7/7 [00:00<00:00,  9.99it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = \".\\\\..\\\\..\\\\data\\\\dataset\\\\UFAL Parallel Corpus of North Levantine 1.0\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_north_levantine_dataset(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line_idx': 0,\n",
       " 'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'دعونا نتحدّث عن الوقت الذي (قدّمت فيه (مالدوفا) لـ(رومانيا ،كعكة عيد ميلاد و (رومانيا) قالت أنّ طعمها جيّد حتّى و إن لم تكن كذلك',\n",
       " 'apc': 'خلونا نحكي عن الوقت اللي قدمت فيه مالدوفا لرومانيا، كعكة عيد ميلاد ورومانيا قالت إنو طيبة حتى لو ما كانت طيبة',\n",
       " 'deu': 'Reden wir darüber, als Moldavien Rumänien einen Geburtstagskuchen machte und Rumänien meinte, er wäre lecker, obwohl er das überhaupt nicht war.',\n",
       " 'ell': 'Ας μιλήσουμε για το όταν η Μολδαβία έκανε στη Ρουμανία μια τούρτα και η Ρουμανία είπε ότι είναι νόστιμη, αν και δεν ήταν.',\n",
       " 'fra': \"Parlons de l'époque Moldova a fait la Roumanie un gâteau d'anniversaire et de la Roumanie a déclaré qu'il avait bon goût même si elle n'a pas fait.\",\n",
       " 'spa': 'Hablemos de la vez que Moldavia le hizo a Rumanía un pastel de cumpleaños y Rumanía dijo que sabía bien aunque no era verdad.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of examples: 120600\n",
      "Features: {'line_idx': Value(dtype='int64', id=None), 'eng': Value(dtype='string', id=None), 'arb': Value(dtype='string', id=None), 'apc': Value(dtype='string', id=None), 'deu': Value(dtype='string', id=None), 'ell': Value(dtype='string', id=None), 'fra': Value(dtype='string', id=None), 'spa': Value(dtype='string', id=None)}\n",
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 62789):\n",
      "  eng: Well, even using the ring will take its toll.\n",
      "  arb: حتى استخدام الخاتم لن يفيدك\n",
      "  apc: حتى استخدام الخاتم مارح يفيدك.\n",
      "\n",
      "Example 2 (Index 38197):\n",
      "  eng: It's good to see that not everybody's put off their dinner by the weather forecast.\n",
      "  arb: إنه من الجيّد رؤية ليس هناك أحد يترك عشائه جراء الأرصاد الجوية.\n",
      "  apc: منيح شوفة ما في حدا بيترك العشا تبعو بسبب الأرصاد الجوية.\n",
      "\n",
      "Example 3 (Index 79060):\n",
      "  eng: Someone on our team took a laptop from Mark's safe house.\n",
      "  arb: شخصٌ ما في فريقنا أخذ حاسوب محمول من منزل (مارك) الأمن\n",
      "  apc: شخص بفريقنا أخد حاسوب من بيت مارك الأمن\n",
      "\n",
      "Example 4 (Index 35791):\n",
      "  eng: You can have him tonight if you like.\n",
      "  arb: يمكنك أن ترينه لليلة إن رغبتِ\n",
      "  apc: فيك تشوفيه لليلة إذا بدك\n",
      "\n",
      "Example 5 (Index 24949):\n",
      "  eng: When did we get this?\n",
      "  arb: متى حصلنا على هذا؟\n",
      "  apc: إيمتى أخدنا هاد؟\n"
     ]
    }
   ],
   "source": [
    "# Print some information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of examples: {len(dataset['train'])}\")\n",
    "print(f\"Features: {dataset['train'].features}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(dataset['train'])), min(5, len(dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    parallel_sentences = get_parallel_sentences(dataset['train'], idx,[\"eng\",\"arb\",\"apc\"])\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    for lang, sentence in parallel_sentences.items():\n",
    "        print(f\"  {lang}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 102510\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 18090\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 102510/102510 [00:01<00:00, 82004.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 18090/18090 [00:00<00:00, 71924.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset locally\n",
    "dataset.save_to_disk(\"../../Data/UFAL Parallel Corpus of North Levantine 1.0/Processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliterate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذهبت إلى المكتبة.\n",
      "ðhbt Ălý Almktbħ.\n",
      "ذهبت إلى المكتبةْ\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "sentence = \"ذهبت إلى المكتبة.\"\n",
    "print(sentence)\n",
    "\n",
    "ar2bw = CharMapper.builtin_mapper('ar2hsb')\n",
    "bw2ar = CharMapper.builtin_mapper('hsb2ar')\n",
    "\n",
    "sent_bw = ar2bw(sentence)\n",
    "sent_ar = bw2ar(sent_bw)\n",
    "print(sent_bw)\n",
    "print(sent_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    return {\"eng\":row[\"eng\"], \"arb\": [ar2bw(ar) for ar in row[\"arb\"]] if isinstance(row[\"arb\"], list) else ar2bw(row[\"arb\"])}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120600/120600 [00:01<00:00, 92329.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test = dataset.map(process_row,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'dEwnA ntHd~v En Alwqt Al*y (qd~mt fyh (mAldwfA) l_(rwmAnyA ،kEkp Eyd mylAd w (rwmAnyA) qAlt >n~ TEmhA jy~d Ht~Y w <n lm tkn k*lk'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    comp = [row[\"eng\"],ar2bw(row[\"arb\"],ar2bw(row[\"apc\"]))] if not isinstance(row[\"arb\"], list) else row[\"eng\"]+[ar2bw(ar) for ar in row[\"arb\"]]+[ar2bw(ar) for ar in row[\"apc\"]]\n",
    "    promp = [\"\"]*len(comp)\n",
    "    return {\"prompt\":promp,\"completion\":comp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    if not isinstance(row[\"arb\"], list):\n",
    "        eng_prompt = [f\"english:{row[\"eng\"]}\\n\"]\n",
    "        arb_prompt = [f\"classical arabic:{ar2bw(row[\"arb\"])}\\n\"]\n",
    "        apc_prompt = [f\"levantine arabic:{ar2bw(row[\"apc\"])}\\n\"]\n",
    "    else:\n",
    "        eng_prompt = [f\"english:{x}\\n\" for x in row[\"eng\"]]\n",
    "        arb_prompt = [f\"classical arabic:{ar2bw(x)}\\n\" for x in row[\"arb\"]]\n",
    "        apc_prompt = [f\"levantine arabic:{ar2bw(x)}\\n\" for x in row[\"apc\"]]\n",
    "    eng_comp = [\"classical arabic:\",\"levantine arabic:\"]  * len(eng_prompt)\n",
    "    arb_comp =  [\"english:\",\"levantine arabic:\"] * len(eng_prompt) \n",
    "    apc_comp = [\"english:\",\"classical arabic:\"] * len(eng_prompt)\n",
    "\n",
    "    prompts = eng_prompt*2+arb_prompt*2+apc_prompt*2\n",
    "    completions = eng_comp+arb_comp+apc_comp\n",
    "    return {\"prompt\":prompts,\"completion\":completions }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 102510/102510 [00:02<00:00, 35560.28 examples/s]\n",
      "Map: 100%|██████████| 18090/18090 [00:00<00:00, 35446.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_test_2 = dataset.map(format_fn,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'english:Massive tits, never speaks.\\n',\n",
       " 'completion': 'levantine arabic:'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_2[\"train\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 307530\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test_2[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 616/616 [00:00<00:00, 1925.02ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.05s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 109/109 [00:00<00:00, 2198.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/KHuss/UFAL_levantine_buckwater_seq2seq/commit/a4a4629be9d3cdcc8ff315c33ef3a2e9a5933079', commit_message='Upload dataset', commit_description='', oid='a4a4629be9d3cdcc8ff315c33ef3a2e9a5933079', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/KHuss/UFAL_levantine_buckwater_seq2seq', endpoint='https://huggingface.co', repo_type='dataset', repo_id='KHuss/UFAL_levantine_buckwater_seq2seq'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_test_2.push_to_hub(\"KHuss/UFAL_levantine_buckwater_seq2seq\",private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
