{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline\n",
    "This notebook is an interactive version of the data processing files, and also allows to explore the dataset interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "This script assume the dataset has already been downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sentences are placed in different files in the source, we haev to match each row from different language files using ids files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_text_file(file_path):\n",
    "    \"\"\"Load a text file and return a list of lines.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "def load_ids_file(file_path):\n",
    "    \"\"\"\n",
    "    Load an .ids file and return a list of mappings.\n",
    "    \n",
    "    Each line in the file is expected to have the format:\n",
    "    source_file\\ttarget_file\\tsource_positions\\ttarget_positions\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with parsed mapping information\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 4:\n",
    "                mapping = {\n",
    "                    'source_file': parts[0],\n",
    "                    'target_file': parts[1],\n",
    "                    'source_positions': parts[2].split(),\n",
    "                    'target_positions': parts[3].split()\n",
    "                }\n",
    "                mappings.append(mapping)\n",
    "            else:\n",
    "                print(f\"Warning: Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "    return mappings\n",
    "\n",
    "def create_north_levantine_dataset(data_dir):\n",
    "    \"\"\"\n",
    "    Create a Hugging Face dataset from the North Levantine parallel corpus.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing the dataset files\n",
    "    \n",
    "    Returns:\n",
    "        A DatasetDict containing the aligned parallel corpus\n",
    "    \"\"\"\n",
    "    # ISO codes for all languages in the dataset\n",
    "    languages = ['eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa']\n",
    "    \n",
    "    # Step 1: Discover and load all language files\n",
    "    text_data = {}\n",
    "    \n",
    "    # Find all language files in the directory\n",
    "    language_files = {}\n",
    "    for file in tqdm(os.listdir(data_dir), desc=\"Finding languages\"):\n",
    "        if file.endswith(tuple(languages)) and not file.endswith(\".ids\"):\n",
    "            lang_code = file.split(\".\")[-1]\n",
    "            language_files[lang_code] = os.path.join(data_dir, file)\n",
    "    \n",
    "    # Load discovered language files\n",
    "    for lang in tqdm(languages, desc=\"Loading language files\"):\n",
    "        if lang in language_files:\n",
    "            text_data[lang] = load_text_file(language_files[lang])\n",
    "            print(f\"Loaded {lang} file with {len(text_data[lang])} lines\")\n",
    "        else:\n",
    "            print(f\"Warning: File for language '{lang}' not found in {data_dir}\")\n",
    "    \n",
    "    # Step 2: Create individual datasets with index column\n",
    "    language_datasets = {}\n",
    "    for lang, lines in tqdm(text_data.items(),desc=\"Creating individual datasets\"):\n",
    "        df = pd.DataFrame({\n",
    "            'text': lines,\n",
    "            'index': list(range(len(lines)))\n",
    "        })\n",
    "        language_datasets[lang] = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Step 3: Create a merged dataset\n",
    "    # Start with English dataset as base\n",
    "    if 'eng' not in language_datasets:\n",
    "        raise ValueError(\"English dataset is required as the base for alignment\")\n",
    "    \n",
    "    # Get the number of examples in the English dataset\n",
    "    num_examples = len(language_datasets['eng'])\n",
    "    \n",
    "    # Verify all languages have the same number of examples\n",
    "    for lang, dataset in language_datasets.items():\n",
    "        if len(dataset) != num_examples:\n",
    "            print(f\"Warning: {lang} dataset has {len(dataset)} examples, but English has {num_examples}\")\n",
    "    \n",
    "    # Create a dictionary for the merged dataset\n",
    "    merged_data = {\n",
    "        'line_idx': list(range(num_examples)),\n",
    "        'eng': language_datasets['eng']['text']\n",
    "    }\n",
    "    \n",
    "    # Add other languages\n",
    "    for lang in languages:\n",
    "        if lang != 'eng' and lang in language_datasets:\n",
    "            if len(language_datasets[lang]) == num_examples:\n",
    "                merged_data[lang] = language_datasets[lang]['text']\n",
    "            else:\n",
    "                # Handle mismatched sizes by padding with empty strings\n",
    "                padded_texts = language_datasets[lang]['text'] + [''] * (num_examples - len(language_datasets[lang]))\n",
    "                merged_data[lang] = padded_texts[:num_examples]\n",
    "    \n",
    "    # Create the merged dataset\n",
    "    merged_dataset = Dataset.from_dict(merged_data)\n",
    "    \n",
    "    # Create a dataset dictionary with train split\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': merged_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset_dict\n",
    "\n",
    "def get_parallel_sentences(dataset, index, languages=None):\n",
    "    \"\"\"\n",
    "    Get parallel sentences for a specific example across all or specified languages.\n",
    "    \n",
    "    Args:\n",
    "        dataset: The dataset containing parallel sentences\n",
    "        index: The index of the example to retrieve\n",
    "        languages: Optional list of language codes to include (default: all available)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping language codes to sentences\n",
    "    \"\"\"\n",
    "    if index >= len(dataset) or index < 0:\n",
    "        raise ValueError(f\"Index {index} out of range (0-{len(dataset)-1})\")\n",
    "    \n",
    "    example = dataset[index]\n",
    "    \n",
    "    if languages is None:\n",
    "        # Get all available languages excluding non-text columns\n",
    "        languages = [col for col in example.keys() if col not in ['line_idx', 'index']]\n",
    "    \n",
    "    return {lang: example[lang] for lang in languages if lang in example}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding languages: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
      "Loading language files:  43%|████▎     | 3/7 [00:00<00:00, 19.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded eng file with 120600 lines\n",
      "Loaded arb file with 120600 lines\n",
      "Loaded apc file with 120600 lines\n",
      "Loaded deu file with 120600 lines\n",
      "Loaded ell file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading language files: 100%|██████████| 7/7 [00:00<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fra file with 120600 lines\n",
      "Loaded spa file with 120600 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating individual datasets: 100%|██████████| 7/7 [00:00<00:00,  9.99it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = \".\\\\..\\\\..\\\\data\\\\dataset\\\\UFAL Parallel Corpus of North Levantine 1.0\"\n",
    "\n",
    "# Create the dataset\n",
    "dataset = create_north_levantine_dataset(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'line_idx': 0,\n",
       " 'eng': \"Let's talk about the time Moldova made Romania a birthday cake and Romania said it tasted good even though it didn't.\",\n",
       " 'arb': 'دعونا نتحدّث عن الوقت الذي (قدّمت فيه (مالدوفا) لـ(رومانيا ،كعكة عيد ميلاد و (رومانيا) قالت أنّ طعمها جيّد حتّى و إن لم تكن كذلك',\n",
       " 'apc': 'خلونا نحكي عن الوقت اللي قدمت فيه مالدوفا لرومانيا، كعكة عيد ميلاد ورومانيا قالت إنو طيبة حتى لو ما كانت طيبة',\n",
       " 'deu': 'Reden wir darüber, als Moldavien Rumänien einen Geburtstagskuchen machte und Rumänien meinte, er wäre lecker, obwohl er das überhaupt nicht war.',\n",
       " 'ell': 'Ας μιλήσουμε για το όταν η Μολδαβία έκανε στη Ρουμανία μια τούρτα και η Ρουμανία είπε ότι είναι νόστιμη, αν και δεν ήταν.',\n",
       " 'fra': \"Parlons de l'époque Moldova a fait la Roumanie un gâteau d'anniversaire et de la Roumanie a déclaré qu'il avait bon goût même si elle n'a pas fait.\",\n",
       " 'spa': 'Hablemos de la vez que Moldavia le hizo a Rumanía un pastel de cumpleaños y Rumanía dijo que sabía bien aunque no era verdad.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information:\n",
      "Number of examples: 120600\n",
      "Features: {'line_idx': Value(dtype='int64', id=None), 'eng': Value(dtype='string', id=None), 'arb': Value(dtype='string', id=None), 'apc': Value(dtype='string', id=None), 'deu': Value(dtype='string', id=None), 'ell': Value(dtype='string', id=None), 'fra': Value(dtype='string', id=None), 'spa': Value(dtype='string', id=None)}\n",
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 62789):\n",
      "  eng: Well, even using the ring will take its toll.\n",
      "  arb: حتى استخدام الخاتم لن يفيدك\n",
      "  apc: حتى استخدام الخاتم مارح يفيدك.\n",
      "\n",
      "Example 2 (Index 38197):\n",
      "  eng: It's good to see that not everybody's put off their dinner by the weather forecast.\n",
      "  arb: إنه من الجيّد رؤية ليس هناك أحد يترك عشائه جراء الأرصاد الجوية.\n",
      "  apc: منيح شوفة ما في حدا بيترك العشا تبعو بسبب الأرصاد الجوية.\n",
      "\n",
      "Example 3 (Index 79060):\n",
      "  eng: Someone on our team took a laptop from Mark's safe house.\n",
      "  arb: شخصٌ ما في فريقنا أخذ حاسوب محمول من منزل (مارك) الأمن\n",
      "  apc: شخص بفريقنا أخد حاسوب من بيت مارك الأمن\n",
      "\n",
      "Example 4 (Index 35791):\n",
      "  eng: You can have him tonight if you like.\n",
      "  arb: يمكنك أن ترينه لليلة إن رغبتِ\n",
      "  apc: فيك تشوفيه لليلة إذا بدك\n",
      "\n",
      "Example 5 (Index 24949):\n",
      "  eng: When did we get this?\n",
      "  arb: متى حصلنا على هذا؟\n",
      "  apc: إيمتى أخدنا هاد؟\n"
     ]
    }
   ],
   "source": [
    "# Print some information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of examples: {len(dataset['train'])}\")\n",
    "print(f\"Features: {dataset['train'].features}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(dataset['train'])), min(5, len(dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    parallel_sentences = get_parallel_sentences(dataset['train'], idx,[\"eng\",\"arb\",\"apc\"])\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    for lang, sentence in parallel_sentences.items():\n",
    "        print(f\"  {lang}: {sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 102510\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['line_idx', 'eng', 'arb', 'apc', 'deu', 'ell', 'fra', 'spa'],\n",
       "        num_rows: 18090\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 102510/102510 [00:01<00:00, 82004.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 18090/18090 [00:00<00:00, 71924.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset locally\n",
    "dataset.save_to_disk(\"../../Data/UFAL Parallel Corpus of North Levantine 1.0/Processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transliterate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذهبت إلى المكتبة.\n",
      "ðhbt Ălý Almktbħ.\n",
      "ذهبت إلى المكتبةْ\n"
     ]
    }
   ],
   "source": [
    "from camel_tools.utils.charmap import CharMapper\n",
    "\n",
    "sentence = \"ذهبت إلى المكتبة.\"\n",
    "print(sentence)\n",
    "\n",
    "ar2bw = CharMapper.builtin_mapper('ar2hsb')\n",
    "bw2ar = CharMapper.builtin_mapper('hsb2ar')\n",
    "\n",
    "sent_bw = ar2bw(sentence)\n",
    "sent_ar = bw2ar(sent_bw)\n",
    "print(sent_bw)\n",
    "print(sent_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    return {\"eng\":row[\"eng\"], \"arb\": [ar2bw(ar) for ar in row[\"arb\"]] if isinstance(row[\"arb\"], list) else ar2bw(row[\"arb\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120600/120600 [00:01<00:00, 88680.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "roman_dataset = dataset.map(format_fn,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 67074):\n",
      "  eng: It's about the Pan Am flight from Lockerbie.\n",
      "  arb: AnhA ttHdθ ςn AxtTAf TAŷrħ lwkrby\n",
      "\n",
      "Example 2 (Index 117225):\n",
      "  eng: I said I'm gonna write a book someday, use all our hard work, take the credit for myself.\n",
      "  arb: qultu sa Ăktb. a ktAb ywmAã mA، Ăstςml. kul~ ςmlnA AlšAq~، xuð. AlfDl fy nfsy.\n",
      "\n",
      "Example 3 (Index 35784):\n",
      "  eng: Then get the hell out of here!\n",
      "  arb: ! ĂðAa Axrji mn hnA\n",
      "\n",
      "Example 4 (Index 44728):\n",
      "  eng: He tried to rape you.\n",
      "  arb: hw HAwla ĂγtiSAbk\n",
      "\n",
      "Example 5 (Index 78549):\n",
      "  eng: I piss on the beards of those selfrighteous monkeys.\n",
      "  arb: Âbwl ςlý lHý hŵlA' Alqrwd Almtςjrfyn\n"
     ]
    }
   ],
   "source": [
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(roman_dataset['train'])), min(5, len(roman_dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    parallel_sentences = get_parallel_sentences(roman_dataset['train'], idx,[\"eng\",\"arb\",\"apc\"])\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    for lang, sentence in parallel_sentences.items():\n",
    "        print(f\"  {lang}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    comp = [row[\"eng\"],ar2bw(row[\"arb\"],ar2bw(row[\"apc\"]))] if not isinstance(row[\"arb\"], list) else row[\"eng\"]+[ar2bw(ar) for ar in row[\"arb\"]]+[ar2bw(ar) for ar in row[\"apc\"]]\n",
    "    promp = [\"\"]*len(comp)\n",
    "    return {\"prompt\":promp,\"completion\":comp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120600/120600 [00:02<00:00, 53256.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "pretrain_dataset = dataset.map(format_fn,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 355528):\n",
      "{'prompt': '', 'completion': 'mA Ălhn Ây Âhmyħ bAlςAlm AlHqyqy'}\n",
      "\n",
      "Example 2 (Index 159478):\n",
      "{'prompt': '', 'completion': 'And if I searched it without one, then any evidence I would find would be inadmissible.'}\n",
      "\n",
      "Example 3 (Index 254657):\n",
      "{'prompt': '', 'completion': 'Poor Mrs. Cross was obliged to accept a paid position in Buckinghamshire.'}\n",
      "\n",
      "Example 4 (Index 113366):\n",
      "{'prompt': '', 'completion': \"It's impossible to steal.\"}\n",
      "\n",
      "Example 5 (Index 42849):\n",
      "{'prompt': '', 'completion': 'knt Âfkr fy jmςhm swyħ.'}\n"
     ]
    }
   ],
   "source": [
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(pretrain_dataset['train'])), min(5, len(pretrain_dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    print(pretrain_dataset[\"train\"][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_fn(row):\n",
    "    prompts = []\n",
    "    completions = []\n",
    "    \n",
    "    # Check if the row values are lists or single strings\n",
    "    if not isinstance(row[\"eng\"], list):\n",
    "        # English to Arabic translations\n",
    "        prompts.append(f\"english: {row['eng']}\\nclassical arabic:\")\n",
    "        completions.append(f\"{row['arb']}\")\n",
    "        \n",
    "        prompts.append(f\"english: {row['eng']}\\nlevantine arabic:\")\n",
    "        completions.append(f\"{row['apc']}\")\n",
    "        \n",
    "        # Classical Arabic to other languages\n",
    "        prompts.append(f\"classical arabic: {row['arb']}\\nenglish:\")\n",
    "        completions.append(f\"{row['eng']}\")\n",
    "        \n",
    "        prompts.append(f\"classical arabic: {row['arb']}\\nlevantine arabic:\")\n",
    "        completions.append(f\"{row['apc']}\")\n",
    "        \n",
    "        # Levantine Arabic to other languages\n",
    "        prompts.append(f\"levantine arabic: {row['apc']}\\nenglish:\")\n",
    "        completions.append(f\"{row['eng']}\")\n",
    "        \n",
    "        prompts.append(f\"levantine arabic: {row['apc']}\\nclassical arabic:\")\n",
    "        completions.append(f\"{row['arb']}\")\n",
    "    else:\n",
    "        # Handle list case (though less likely in this context)\n",
    "        for i in range(len(row[\"eng\"])):\n",
    "            # English to Arabic translations\n",
    "            prompts.append(f\"english: {row['eng'][i]}\\nclassical arabic:\")\n",
    "            completions.append(f\"{row['arb'][i]}\")\n",
    "            \n",
    "            prompts.append(f\"english: {row['eng'][i]}\\nlevantine arabic:\")\n",
    "            completions.append(f\"{row['apc'][i]}\")\n",
    "            \n",
    "            # Classical Arabic to other languages\n",
    "            prompts.append(f\"classical arabic: {row['arb'][i]}\\nenglish:\")\n",
    "            completions.append(f\"{row['eng'][i]}\")\n",
    "            \n",
    "            prompts.append(f\"classical arabic: {row['arb'][i]}\\nlevantine arabic:\")\n",
    "            completions.append(f\"{row['apc'][i]}\")\n",
    "            \n",
    "            # Levantine Arabic to other languages\n",
    "            prompts.append(f\"levantine arabic: {row['apc'][i]}\\nenglish:\")\n",
    "            completions.append(f\"{row['eng'][i]}\")\n",
    "            \n",
    "            prompts.append(f\"levantine arabic: {row['apc'][i]}\\nclassical arabic:\")\n",
    "            completions.append(f\"{row['arb'][i]}\")\n",
    "    \n",
    "    return {\"prompt\": prompts, \"completion\": completions}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120600/120600 [00:01<00:00, 81307.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "translation_dataset = dataset.map(format_fn,remove_columns=dataset[\"train\"].column_names,batched=True,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing 5 random examples:\n",
      "\n",
      "Example 1 (Index 288211):\n",
      "{'prompt': 'english: Here is a hot beverage to comfort you.\\nlevantine arabic:', 'completion': 'هاد مشروب ساخن لترتاح'}\n",
      "\n",
      "Example 2 (Index 203638):\n",
      "{'prompt': 'levantine arabic: إنتي ملكة وكلشي بتحكميه هالك\\nenglish:', 'completion': 'Kill a queen, and all queens are mortal.'}\n",
      "\n",
      "Example 3 (Index 604281):\n",
      "{'prompt': 'classical arabic: لاأريد أن أجرح مشاعرك\\nlevantine arabic:', 'completion': 'ما بدي أجرح مشاعرك.'}\n",
      "\n",
      "Example 4 (Index 643991):\n",
      "{'prompt': 'levantine arabic: أكتر شي فينا نساويه أنو نوصلك لهنيك، و.. حتى أقدر عيش بسجن؟\\nclassical arabic:', 'completion': 'جلماعلينافعلههوإيصالكهناك, و. . لأتمكن من العيش في زنزانه ؟'}\n",
      "\n",
      "Example 5 (Index 721383):\n",
      "{'prompt': 'classical arabic: (كان يجب ان اخبرك عن (اليكس\\nlevantine arabic:', 'completion': 'كان لازم خبرك عن أليكس'}\n"
     ]
    }
   ],
   "source": [
    "# Show some examples\n",
    "print(f\"\\nShowing {5} random examples:\")\n",
    "import random\n",
    "sample_indices = random.sample(range(len(translation_dataset['train'])), min(5, len(translation_dataset['train'])))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\nExample {i+1} (Index {idx}):\")\n",
    "    print(translation_dataset[\"train\"][idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
